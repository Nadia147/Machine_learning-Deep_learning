{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nadia147/Machine_learning-Deep_learning/blob/main/change_win_3dcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W2WRHZv6z95j",
        "outputId": "ef1a4fdc-23c1-4495-b2d2-dfa60354ed59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting earthpy\n",
            "  Downloading earthpy-0.9.4-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (from earthpy) (0.13.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from earthpy) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from earthpy) (1.23.5)\n",
            "Collecting rasterio (from earthpy)\n",
            "  Downloading rasterio-1.3.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from earthpy) (0.19.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from earthpy) (2.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (2.8.2)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas->earthpy) (1.9.4.post1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from geopandas->earthpy) (1.5.3)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas->earthpy) (3.6.0)\n",
            "Requirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas->earthpy) (2.0.1)\n",
            "Collecting affine (from rasterio->earthpy)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (2023.7.22)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (8.1.6)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (0.7.2)\n",
            "Collecting snuggs>=1.4.1 (from rasterio->earthpy)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->earthpy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->earthpy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->earthpy) (2.0.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (2023.7.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->earthpy) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas->earthpy) (2023.3)\n",
            "Installing collected packages: snuggs, affine, rasterio, earthpy\n",
            "Successfully installed affine-2.4.0 earthpy-0.9.4 rasterio-1.3.8 snuggs-1.4.7\n",
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.23.5)\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.23.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "! pip install earthpy\n",
        "# Colab Requirements\n",
        "!pip install spectral\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "## Basics\n",
        "import gc\n",
        "gc.collect()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "import numpy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from operator import truediv\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import spectral\n",
        "## Ploting\n",
        "from plotly.offline import init_notebook_mode\n",
        "import plotly.graph_objs as go\n",
        "from matplotlib.pyplot import cm\n",
        "import matplotlib.pyplot as plt\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline\n",
        "## Sklearn\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "## Deep Model\n",
        "import keras\n",
        "import h5py\n",
        "from keras.layers import Dropout, Input, Conv2D, Conv3D, MaxPool3D, Flatten, Dense, Reshape, BatchNormalization\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.models import Sequential, Model\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "## Mounting Colab\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import earthpy.plot as ep\n",
        "import seaborn as sns\n",
        "import earthpy.spatial as es\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "from scipy.io import loadmat\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             confusion_matrix, classification_report)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from tqdm import tqdm\n",
        "import scipy.io"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6hfBYwnKG2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAQoE7gbyk2Y",
        "outputId": "db648aa6-6d23-48fe-e0b1-f9e1778dfce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 9, 9, 20, 1)]     0         \n",
            "                                                                 \n",
            " conv3d (Conv3D)             (None, 7, 7, 14, 8)       512       \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 5488)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               1405184   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                2064      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,440,656\n",
            "Trainable params: 1,440,656\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 2459 samples, validate on 1640 samples\n",
            "Epoch 1/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 244.8798 - acc: 0.2403 - val_loss: 27.4340 - val_acc: 0.5354\n",
            "Epoch 2/50\n",
            "2459/2459 [==============================] - 6s 2ms/sample - loss: 37.6766 - acc: 0.2940 - val_loss: 3.3210 - val_acc: 0.3140\n",
            "Epoch 3/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 4.0846 - acc: 0.2098 - val_loss: 2.6750 - val_acc: 0.2195\n",
            "Epoch 4/50\n",
            "2459/2459 [==============================] - 4s 1ms/sample - loss: 2.7614 - acc: 0.2285 - val_loss: 2.6515 - val_acc: 0.2957\n",
            "Epoch 5/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.6900 - acc: 0.2379 - val_loss: 2.6362 - val_acc: 0.2878\n",
            "Epoch 6/50\n",
            "2459/2459 [==============================] - 6s 2ms/sample - loss: 2.6566 - acc: 0.2529 - val_loss: 2.5797 - val_acc: 0.2933\n",
            "Epoch 7/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.6144 - acc: 0.2631 - val_loss: 2.5639 - val_acc: 0.2933\n",
            "Epoch 8/50\n",
            "2459/2459 [==============================] - 4s 1ms/sample - loss: 2.5883 - acc: 0.2643 - val_loss: 2.5502 - val_acc: 0.2988\n",
            "Epoch 9/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 2.6171 - acc: 0.2737 - val_loss: 2.5227 - val_acc: 0.2951\n",
            "Epoch 10/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.5736 - acc: 0.2639 - val_loss: 2.5144 - val_acc: 0.2927\n",
            "Epoch 11/50\n",
            "2459/2459 [==============================] - 4s 1ms/sample - loss: 2.5638 - acc: 0.2696 - val_loss: 2.5041 - val_acc: 0.2933\n",
            "Epoch 12/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.5514 - acc: 0.2700 - val_loss: 2.4883 - val_acc: 0.2945\n",
            "Epoch 13/50\n",
            "2459/2459 [==============================] - 6s 2ms/sample - loss: 2.5465 - acc: 0.2660 - val_loss: 2.5159 - val_acc: 0.2805\n",
            "Epoch 14/50\n",
            "2459/2459 [==============================] - 4s 1ms/sample - loss: 2.5427 - acc: 0.2623 - val_loss: 2.4685 - val_acc: 0.3006\n",
            "Epoch 15/50\n",
            "2459/2459 [==============================] - 4s 1ms/sample - loss: 2.4837 - acc: 0.2834 - val_loss: 2.4961 - val_acc: 0.3067\n",
            "Epoch 16/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 2.4925 - acc: 0.2887 - val_loss: 2.4111 - val_acc: 0.3067\n",
            "Epoch 17/50\n",
            "2459/2459 [==============================] - 7s 3ms/sample - loss: 2.4678 - acc: 0.2806 - val_loss: 2.4245 - val_acc: 0.2970\n",
            "Epoch 18/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.4671 - acc: 0.2802 - val_loss: 2.4112 - val_acc: 0.2988\n",
            "Epoch 19/50\n",
            "2459/2459 [==============================] - 6s 2ms/sample - loss: 2.4553 - acc: 0.2757 - val_loss: 2.3967 - val_acc: 0.3030\n",
            "Epoch 20/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.4180 - acc: 0.2875 - val_loss: 2.4069 - val_acc: 0.3067\n",
            "Epoch 21/50\n",
            "2459/2459 [==============================] - 4s 1ms/sample - loss: 2.4167 - acc: 0.2875 - val_loss: 2.3907 - val_acc: 0.3098\n",
            "Epoch 22/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.4081 - acc: 0.2863 - val_loss: 2.3757 - val_acc: 0.3073\n",
            "Epoch 23/50\n",
            "2459/2459 [==============================] - 6s 2ms/sample - loss: 2.3992 - acc: 0.2830 - val_loss: 2.3641 - val_acc: 0.3049\n",
            "Epoch 24/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3950 - acc: 0.2830 - val_loss: 2.3631 - val_acc: 0.3073\n",
            "Epoch 25/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3935 - acc: 0.2891 - val_loss: 2.3362 - val_acc: 0.3079\n",
            "Epoch 26/50\n",
            "2459/2459 [==============================] - 6s 2ms/sample - loss: 2.4171 - acc: 0.2855 - val_loss: 2.3354 - val_acc: 0.3006\n",
            "Epoch 27/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3777 - acc: 0.2765 - val_loss: 2.3205 - val_acc: 0.3073\n",
            "Epoch 28/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3828 - acc: 0.2794 - val_loss: 2.3269 - val_acc: 0.3098\n",
            "Epoch 29/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 2.3866 - acc: 0.2834 - val_loss: 2.3021 - val_acc: 0.3067\n",
            "Epoch 30/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 2.3980 - acc: 0.2692 - val_loss: 2.3338 - val_acc: 0.2921\n",
            "Epoch 31/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3807 - acc: 0.2733 - val_loss: 2.3104 - val_acc: 0.2976\n",
            "Epoch 32/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.4049 - acc: 0.2761 - val_loss: 2.2914 - val_acc: 0.3085\n",
            "Epoch 33/50\n",
            "2459/2459 [==============================] - 7s 3ms/sample - loss: 2.3657 - acc: 0.2741 - val_loss: 2.3001 - val_acc: 0.3104\n",
            "Epoch 34/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3278 - acc: 0.2826 - val_loss: 2.3076 - val_acc: 0.3104\n",
            "Epoch 35/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3409 - acc: 0.2761 - val_loss: 2.2741 - val_acc: 0.3085\n",
            "Epoch 36/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 2.3304 - acc: 0.2814 - val_loss: 2.2932 - val_acc: 0.3098\n",
            "Epoch 37/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 2.3419 - acc: 0.2786 - val_loss: 2.2726 - val_acc: 0.3091\n",
            "Epoch 38/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3372 - acc: 0.2721 - val_loss: 2.2568 - val_acc: 0.3024\n",
            "Epoch 39/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3357 - acc: 0.2733 - val_loss: 2.2581 - val_acc: 0.3006\n",
            "Epoch 40/50\n",
            "2459/2459 [==============================] - 6s 2ms/sample - loss: 2.3327 - acc: 0.2729 - val_loss: 2.2479 - val_acc: 0.3024\n",
            "Epoch 41/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3303 - acc: 0.2712 - val_loss: 2.2403 - val_acc: 0.3030\n",
            "Epoch 42/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.3225 - acc: 0.2741 - val_loss: 2.2351 - val_acc: 0.3037\n",
            "Epoch 43/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 2.3162 - acc: 0.2737 - val_loss: 2.2186 - val_acc: 0.3073\n",
            "Epoch 44/50\n",
            "2459/2459 [==============================] - 5s 2ms/sample - loss: 2.3206 - acc: 0.2741 - val_loss: 2.3839 - val_acc: 0.3104\n",
            "Epoch 45/50\n",
            "2459/2459 [==============================] - 4s 1ms/sample - loss: 2.3101 - acc: 0.2839 - val_loss: 2.2089 - val_acc: 0.3079\n",
            "Epoch 46/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.2994 - acc: 0.2769 - val_loss: 2.2113 - val_acc: 0.3067\n",
            "Epoch 47/50\n",
            "2459/2459 [==============================] - 7s 3ms/sample - loss: 2.2985 - acc: 0.2769 - val_loss: 2.2109 - val_acc: 0.3049\n",
            "Epoch 48/50\n",
            "2459/2459 [==============================] - 4s 1ms/sample - loss: 2.2975 - acc: 0.2733 - val_loss: 2.2027 - val_acc: 0.3067\n",
            "Epoch 49/50\n",
            "2459/2459 [==============================] - 4s 2ms/sample - loss: 2.2922 - acc: 0.2790 - val_loss: 2.1986 - val_acc: 0.3104\n",
            "Epoch 50/50\n",
            "2459/2459 [==============================] - 6s 2ms/sample - loss: 2.2779 - acc: 0.2814 - val_loss: 2.1933 - val_acc: 0.3098\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Split Training, Validation and Test Sets\n",
        "def SplitTr_Te(HSI, GT, TeRatio, randomState=345):\n",
        "    Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=TeRatio, random_state=randomState, stratify=GT)\n",
        "    return Tr, Te, TrC, TeC\n",
        "def DL_Method(HSI, numComponents = 75):\n",
        "    RHSI = np.reshape(HSI, (-1, HSI.shape[2]))\n",
        "    n_batches = 256\n",
        "    inc_pca = IncrementalPCA(n_components=numComponents)\n",
        "    for X_batch in np.array_split(RHSI, n_batches):\n",
        "        inc_pca.partial_fit(X_batch)\n",
        "    X_ipca = inc_pca.transform(RHSI)\n",
        "    RHSI = np.reshape(X_ipca, (HSI.shape[0],HSI.shape[1], numComponents))\n",
        "    return RHSI\n",
        "## Padding\n",
        "def ZeroPad(HSI, margin=2):\n",
        "    NHSI = np.zeros((HSI.shape[0] + 2 * margin, HSI.shape[1] + 2* margin, HSI.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    NHSI[x_offset:HSI.shape[0] + x_offset, y_offset:HSI.shape[1] + y_offset, :] = HSI\n",
        "    return NHSI\n",
        "## Spatial Patches in 3D\n",
        "def HSICubes(HSI, GT, WinSize=5, removeZeroLabels = True):#5t01\n",
        "    margin = int((WinSize - 1) / 2)\n",
        "    zeroPaddedX = ZeroPad(HSI, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((HSI.shape[0] * HSI.shape[1], WinSize, WinSize, HSI.shape[2]))\n",
        "    patchesLabels = np.zeros((HSI.shape[0] * HSI.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = GT[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n",
        "## Compute Per Class Accuacy form Confusion Matrix\n",
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc\n",
        "## Assigning Class Labels and Computing the Test Accuracy\n",
        "def reports(Te,TeC):\n",
        "    start = time.time()\n",
        "    Y_pred = model.predict(Te)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    end = time.time()\n",
        "    total = end - start\n",
        "    target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
        "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed',\n",
        "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
        "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
        "                        'Stone-Steel-Towers']\n",
        "    classification = classification_report(np.argmax(TeC, axis=1), y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(np.argmax(TeC, axis=1), y_pred)\n",
        "    confusion = confusion_matrix(np.argmax(TeC, axis=1), y_pred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(TeC, axis=1), y_pred)\n",
        "    score = model.evaluate(Te, TeC, batch_size=32)\n",
        "    Test_Loss =  score[0]*100\n",
        "    Test_accuracy = score[1]*100\n",
        "    return classification, confusion, Test_Loss, Test_accuracy, oa*100, each_acc*100, aa*100, kappa*100, target_names, y_pred, total\n",
        "## Compute the Patch to Prepare for Ground Truths\n",
        "def Patch(data,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "    patch = data[height_slice, width_slice, :]\n",
        "    return patch\n",
        "## Loading Dataset\n",
        "import scipy.io\n",
        "## There is a typo in the paper on page#4 Table#4, the training samples are not 10%, the reported accuracies are obtained with almost 30~40% training samples.\n",
        "#data_path = os.path.join(os.getcwd(),'/content/drive/My Drive/Colab Notebooks')\n",
        "#HSI = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
        "HSI = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/Indian_pines_corrected.mat')['indian_pines_corrected'];\n",
        "#GT = labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
        "GT = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/Indian_pines_gt.mat')['indian_pines_gt']\n",
        "HSI.shape, GT.shape\n",
        "## Reduce the Dimensionality\n",
        "HSI = DL_Method(HSI, numComponents=20)\n",
        "HSI.shape\n",
        "## Create Image Cubes for Model Building\n",
        "HSI, GT = HSICubes(HSI, GT, WinSize=9)\n",
        "HSI.shape, GT.shape\n",
        "## Split Train and Test sets\n",
        "Tr, Te, TrC, TeC = SplitTr_Te(HSI, GT, 0.60)\n",
        "Tr.shape, Te.shape, TrC.shape, TeC.shape\n",
        "## Split Train and Validation\n",
        "Tr, Tv, TrC, TvC = SplitTr_Te(Tr, TrC, 0.40)\n",
        "Tr.shape, Tv.shape, TrC.shape, TvC.shape\n",
        "\n",
        "Tr = Tr.reshape(-1, 9, 9, 20, 1)\n",
        "TrC = np_utils.to_categorical(TrC)\n",
        "Tv = Tv.reshape(-1, 9, 9, 20, 1)\n",
        "TvC = np_utils.to_categorical(TvC)\n",
        "Tr.shape, TrC.shape, Tv.shape, TvC.shape\n",
        "\n",
        "## Model Structure\n",
        "## Input layer\n",
        "input_layer = Input((9, 9, 20, 1))\n",
        "## 2D Convolutional Layers\n",
        "conv_layer1 = Conv2D(filters=128, kernel_size=(1,1), activation='relu')(input_layer)\n",
        "#conv_layer2 = Conv2D(filters=16, kernel_size=(1,1), activation='relu')(conv_layer1)\n",
        "#conv_layer3 = Conv2D(filters=32, kernel_size=(1,1), activation='relu')(conv_layer2)\n",
        "#conv_layer4 = Conv2D(filters=64, kernel_size=(1,1), activation='relu')(conv_layer3)\n",
        "\n",
        "## 3D Convolutional Layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3,3,7), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3,3,5), activation='relu')(conv_layer1)\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3,3,3), activation='relu')(conv_layer2)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(3,3,3), activation='relu')(conv_layer3)\n",
        "\n",
        "\n",
        "## Faltten 3D Convolutional Layer\n",
        "flatten_layer = Flatten()(conv_layer1)#4to1\n",
        "## Fully Connected Layers\n",
        "dense_layer1 = Dense(units=256, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "output_layer = Dense(units=16, activation='softmax')(dense_layer2)\n",
        "\n",
        "# define the model with input layer and output layer\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()\n",
        "# compiling the model\n",
        "adam = Adam(lr=0.001, decay=1e-06)#change .001 to .01\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "## Training Time and Fit the Model\n",
        "start = time.time()\n",
        "#history = model.fit(x=Tr, y=TrC, batch_size=256, epochs=50, validation_data=(Tv, TvC))#ep=50 t020\n",
        "#end = time.time()\n",
        "#Tr_Time = end - start\n",
        "history = model.fit(x=Tr, y=TrC, batch_size=256, epochs=50, validation_data=(Tv, TvC))  # ep=50 t020\n",
        "#end = time.time()\n",
        "#Tr_Time = end - start"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Load your hyperspectral data and ground truth\n",
        "#HSI = np.load('your_hsi_data.npy')  # Replace with your data file\n",
        "#GT = np.load('your_ground_truth.npy')  # Replace with your ground truth file\n",
        "HSI = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/Indian_pines_corrected.mat')['indian_pines_corrected'];\n",
        "#GT = labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
        "GT = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/Indian_pines_gt.mat')['indian_pines_gt']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(9, 9, HSI.shape[2])))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=np.unique(GT).shape[0], activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "adam = Adam(lr=0.001, decay=1e-06)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "# Reshape and preprocess the data\n",
        "Tr = Tr.reshape(-1, 9, 9, HSI.shape[2])\n",
        "TrC = np_utils.to_categorical(TrC)\n",
        "\n",
        "# Train the model\n",
        "model.fit(x=Tr, y=TrC, batch_size=64, epochs=20, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "Te = Te.reshape(-1, 9, 9, HSI.shape[2])\n",
        "TeC = np_utils.to_categorical(TeC)\n",
        "score = model.evaluate(Te, TeC, batch_size=32)\n",
        "print(\"Test Loss:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "9sZKVqFAq6zS",
        "outputId": "ee7cc999-c716-4c5d-c607-ae424249ab33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ccaf9c179cac>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Reshape and preprocess the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mTr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHSI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mTrC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3364000 into shape (9,9,200)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KpWWndiraKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, cohen_kappa_score\n",
        "from keras.layers import Input, Conv2D, Conv3D, Flatten, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Split Training, Validation, and Test Sets\n",
        "def SplitTr_Te(HSI, GT, TeRatio, randomState=345):\n",
        "    Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=TeRatio, random_state=randomState, stratify=GT)\n",
        "    return Tr, Te, TrC, TeC\n",
        "\n",
        "# ... (other functions remain the same)\n",
        "\n",
        "## Create Image Cubes for Model Building\n",
        "HSI, GT = HSICubes(HSI, GT, WinSize=13)\n",
        "HSI.shape, GT.shape\n",
        "\n",
        "## Split Train and Test sets\n",
        "Tr, Te, TrC, TeC = SplitTr_Te(HSI, GT, 0.60)\n",
        "Tr.shape, Te.shape, TrC.shape, TeC.shape\n",
        "\n",
        "## Split Train and Validation\n",
        "Tr, Tv, TrC, TvC = SplitTr_Te(Tr, TrC, 0.30)\n",
        "Tr.shape, Tv.shape, TrC.shape, TvC.shape\n",
        "\n",
        "## Model Pre requisites\n",
        "Tr = Tr.reshape(-1, 13, 13, 13, 1)\n",
        "TrC = np_utils.to_categorical(TrC)\n",
        "Tv = Tv.reshape(-1, 13, 13, 13, 1)\n",
        "TvC = np_utils.to_categorical(TvC)\n",
        "Tr.shape, TrC.shape, Tv.shape, TvC.shape\n",
        "\n",
        "## Model Structure\n",
        "## Input layer\n",
        "input_layer = Input((13, 13, 13, 1))\n",
        "\n",
        "## 3D Convolutional Layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(conv_layer2)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer3)\n",
        "\n",
        "## Faltten 3D Convolutional Layer\n",
        "flatten_layer = Flatten()(conv_layer1)\n",
        "\n",
        "## Fully Connected Layers\n",
        "dense_layer1 = Dense(units=256, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "output_layer = Dense(units=16, activation='softmax')(dense_layer2)\n",
        "\n",
        "# Define the model with input layer and output layer\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()\n",
        "\n",
        "# Compiling the model\n",
        "adam = Adam(lr=0.001, decay=1e-06)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "## Training Time and Fit the Model\n",
        "start = time.time()\n",
        "history = model.fit(x=Tr, y=TrC, batch_size=256, epochs=50, validation_data=(Tv, TvC))\n",
        "end = time.time()\n",
        "Tr_Time = end - start\n"
      ],
      "metadata": {
        "id": "xqGUxDkoHGHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split Training, Validation and Test Sets\n",
        "def SplitTr_Te(HSI, GT, TeRatio, randomState=345):\n",
        "    Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=TeRatio, random_state=randomState, stratify=GT)\n",
        "    return Tr, Te, TrC, TeC\n",
        "def DL_Method(HSI, numComponents = 75):\n",
        "    RHSI = np.reshape(HSI, (-1, HSI.shape[2]))\n",
        "    n_batches = 256\n",
        "    inc_pca = IncrementalPCA(n_components=numComponents)\n",
        "    for X_batch in np.array_split(RHSI, n_batches):\n",
        "        inc_pca.partial_fit(X_batch)\n",
        "    X_ipca = inc_pca.transform(RHSI)\n",
        "    RHSI = np.reshape(X_ipca, (HSI.shape[0],HSI.shape[1], numComponents))\n",
        "    return RHSI\n",
        "## Padding\n",
        "def ZeroPad(HSI, margin=2):\n",
        "    NHSI = np.zeros((HSI.shape[0] + 2 * margin, HSI.shape[1] + 2* margin, HSI.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    NHSI[x_offset:HSI.shape[0] + x_offset, y_offset:HSI.shape[1] + y_offset, :] = HSI\n",
        "    return NHSI\n",
        "## Spatial Patches in 3D\n",
        "def HSICubes(HSI, GT, WinSize=6, removeZeroLabels = True):#5t01\n",
        "    margin = int((WinSize - 1) / 2)\n",
        "    zeroPaddedX = ZeroPad(HSI, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((HSI.shape[0] * HSI.shape[1], WinSize, WinSize, HSI.shape[2]))\n",
        "    patchesLabels = np.zeros((HSI.shape[0] * HSI.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = GT[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n",
        "## Compute Per Class Accuacy form Confusion Matrix\n",
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc\n",
        "## Assigning Class Labels and Computing the Test Accuracy\n",
        "def reports(Te,TeC):\n",
        "    start = time.time()\n",
        "    Y_pred = model.predict(Te)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    end = time.time()\n",
        "    total = end - start\n",
        "    target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
        "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed',\n",
        "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
        "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
        "                        'Stone-Steel-Towers']\n",
        "    classification = classification_report(np.argmax(TeC, axis=1), y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(np.argmax(TeC, axis=1), y_pred)\n",
        "    confusion = confusion_matrix(np.argmax(TeC, axis=1), y_pred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(TeC, axis=1), y_pred)\n",
        "    score = model.evaluate(Te, TeC, batch_size=32)\n",
        "    Test_Loss =  score[0]*100\n",
        "    Test_accuracy = score[1]*100\n",
        "    return classification, confusion, Test_Loss, Test_accuracy, oa*100, each_acc*100, aa*100, kappa*100, target_names, y_pred, total\n",
        "## Compute the Patch to Prepare for Ground Truths\n",
        "def Patch(data,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "    patch = data[height_slice, width_slice, :]\n",
        "    return patch\n",
        "## Loading Dataset\n",
        "import scipy.io\n",
        "## There is a typo in the paper on page#4 Table#4, the training samples are not 10%, the reported accuracies are obtained with almost 30~40% training samples.\n",
        "#data_path = os.path.join(os.getcwd(),'/content/drive/My Drive/Colab Notebooks')\n",
        "#HSI = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
        "HSI = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/Indian_pines_corrected.mat')['indian_pines_corrected'];\n",
        "#GT = labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
        "GT = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/Indian_pines_gt.mat')['indian_pines_gt']\n",
        "HSI.shape, GT.shape\n",
        "## Reduce the Dimensionality\n",
        "HSI = DL_Method(HSI, numComponents=20)\n",
        "HSI.shape\n",
        "## Create Image Cubes for Model Building\n",
        "HSI, GT = HSICubes(HSI, GT, WinSize=13)\n",
        "HSI.shape, GT.shape\n",
        "## Split Train and Test sets\n",
        "Tr, Te, TrC, TeC = SplitTr_Te(HSI, GT, 0.60)\n",
        "Tr.shape, Te.shape, TrC.shape, TeC.shape\n",
        "## Split Train and Validation\n",
        "Tr, Tv, TrC, TvC = SplitTr_Te(Tr, TrC, 0.40)\n",
        "Tr.shape, Tv.shape, TrC.shape, TvC.shape\n",
        "## Model Pre requsites\n",
        "Tr = Tr.reshape(-1, 13, 13, 20, 1)\n",
        "TrC = np_utils.to_categorical(TrC)\n",
        "Tv = Tv.reshape(-1, 13, 13, 20, 1)\n",
        "TvC = np_utils.to_categorical(TvC)\n",
        "Tr.shape, TrC.shape, Tv.shape, TvC.shape\n",
        "\n",
        "## Model Structure\n",
        "## Input layer\n",
        "input_layer = Input((13, 13, 20, 1))\n",
        "## 2D Convolutional Layers\n",
        "conv_layer1 = Conv2D(filters=8, kernel_size=(1,1), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv2D(filters=16, kernel_size=(1,1), activation='relu')(conv_layer1)\n",
        "conv_layer3 = Conv2D(filters=32, kernel_size=(1,1), activation='relu')(conv_layer2)\n",
        "conv_layer4 = Conv2D(filters=64, kernel_size=(1,1), activation='relu')(conv_layer3)\n",
        "\n",
        "## 3D Convolutional Layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3,3,7), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3,3,5), activation='relu')(conv_layer1)\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3,3,3), activation='relu')(conv_layer2)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(3,3,3), activation='relu')(conv_layer3)\n",
        "\n",
        "\n",
        "## Faltten 3D Convolutional Layer\n",
        "flatten_layer = Flatten()(conv_layer1)#4to1\n",
        "## Fully Connected Layers\n",
        "dense_layer1 = Dense(units=256, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "output_layer = Dense(units=16, activation='softmax')(dense_layer2)\n",
        "\n",
        "# define the model with input layer and output layer\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()\n",
        "# compiling the model\n",
        "adam = Adam(lr=0.001, decay=1e-06)#change .001 to .01\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "## Training Time and Fit the Model\n",
        "start = time.time()\n",
        "#history = model.fit(x=Tr, y=TrC, batch_size=256, epochs=50, validation_data=(Tv, TvC))#ep=50 t020\n",
        "#end = time.time()\n",
        "#Tr_Time = end - start\n",
        "history = model.fit(x=Tr, y=TrC, batch_size=256, epochs=50, validation_data=(Tv, TvC))  # ep=50 t020\n",
        "#end = time.time()\n",
        "#Tr_Time = end - start"
      ],
      "metadata": {
        "id": "4eSBMy2Cra-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQXbPw67jYXohpac9KklZ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}