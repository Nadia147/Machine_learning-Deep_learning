{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nadia147/Machine_learning-Deep_learning/blob/main/FINAL_2d3d_PU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cpGB0cIbLi0v",
        "outputId": "60a46cc4-76a1-48f3-951d-456bba68c1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting earthpy\n",
            "  Downloading earthpy-0.9.4-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (from earthpy) (0.13.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from earthpy) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from earthpy) (1.23.5)\n",
            "Collecting rasterio (from earthpy)\n",
            "  Downloading rasterio-1.3.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from earthpy) (0.19.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from earthpy) (2.31.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->earthpy) (2.8.2)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas->earthpy) (1.9.4.post1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from geopandas->earthpy) (1.5.3)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas->earthpy) (3.6.0)\n",
            "Requirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas->earthpy) (2.0.1)\n",
            "Collecting affine (from rasterio->earthpy)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (2023.7.22)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (0.7.2)\n",
            "Collecting snuggs>=1.4.1 (from rasterio->earthpy)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio->earthpy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->earthpy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->earthpy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->earthpy) (2.0.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (2023.8.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->earthpy) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->earthpy) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas->earthpy) (2023.3)\n",
            "Installing collected packages: snuggs, affine, rasterio, earthpy\n",
            "Successfully installed affine-2.4.0 earthpy-0.9.4 rasterio-1.3.8 snuggs-1.4.7\n",
            "Collecting spectral\n",
            "  Downloading spectral-0.23.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spectral) (1.23.5)\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.23.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "! pip install earthpy\n",
        "# Colab Requirements\n",
        "!pip install spectral\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "## Basics\n",
        "import gc\n",
        "gc.collect()\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "import numpy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from operator import truediv\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import spectral\n",
        "## Ploting\n",
        "from plotly.offline import init_notebook_mode\n",
        "import plotly.graph_objs as go\n",
        "from matplotlib.pyplot import cm\n",
        "import matplotlib.pyplot as plt\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline\n",
        "## Sklearn\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "## Deep Model\n",
        "import keras\n",
        "import h5py\n",
        "from keras.layers import Dropout, Input, Conv2D, Conv3D, MaxPool3D, Flatten, Dense, Reshape, BatchNormalization\n",
        "from plotly.offline import iplot, init_notebook_mode\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.models import Sequential, Model\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "## Mounting Colab\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import earthpy.plot as ep\n",
        "import seaborn as sns\n",
        "import earthpy.spatial as es\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "from scipy.io import loadmat\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             confusion_matrix, classification_report)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from tqdm import tqdm\n",
        "import scipy.io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nr9Qc7ULkMQ"
      },
      "outputs": [],
      "source": [
        "## Split Training, Validation and Test Sets\n",
        "def SplitTr_Te(HSI, GT, TeRatio, randomState=345):\n",
        "    Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=TeRatio, random_state=randomState, stratify=GT)\n",
        "    return Tr, Te, TrC, TeC\n",
        "def DL_Method(HSI, numComponents = 75):\n",
        "    RHSI = np.reshape(HSI, (-1, HSI.shape[2]))\n",
        "    n_batches = 256\n",
        "    inc_pca = IncrementalPCA(n_components=numComponents)\n",
        "    for X_batch in np.array_split(RHSI, n_batches):\n",
        "        inc_pca.partial_fit(X_batch)\n",
        "    X_ipca = inc_pca.transform(RHSI)\n",
        "    RHSI = np.reshape(X_ipca, (HSI.shape[0],HSI.shape[1], numComponents))\n",
        "    return RHSI\n",
        "## Padding\n",
        "def ZeroPad(HSI, margin=2):\n",
        "    NHSI = np.zeros((HSI.shape[0] + 2 * margin, HSI.shape[1] + 2* margin, HSI.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    NHSI[x_offset:HSI.shape[0] + x_offset, y_offset:HSI.shape[1] + y_offset, :] = HSI\n",
        "    return NHSI\n",
        "## Spatial Patches in 3D\n",
        "def HSICubes(HSI, GT, WinSize=5, removeZeroLabels = True):#5t01\n",
        "    margin = int((WinSize - 1) / 2)\n",
        "    zeroPaddedX = ZeroPad(HSI, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((HSI.shape[0] * HSI.shape[1], WinSize, WinSize, HSI.shape[2]))\n",
        "    patchesLabels = np.zeros((HSI.shape[0] * HSI.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = GT[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n",
        "## Compute Per Class Accuacy form Confusion Matrix\n",
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, cohen_kappa_score\n",
        "\n",
        "def evaluate_performance(model, test_data, test_labels):\n",
        "    start = time.time()\n",
        "    Y_pred = model.predict(test_data)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    end = time.time()\n",
        "    total_time = end - start\n",
        "\n",
        "    target_names = ['Asphalt', 'Meadows', 'Gravel', 'Trees', 'Metal Sheet',\n",
        "                    'Bare Soil', 'Bitumen', 'Brick', 'Shadow']\n",
        "\n",
        "    classification = classification_report(np.argmax(test_labels, axis=1), y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(np.argmax(test_labels, axis=1), y_pred)\n",
        "    confusion = confusion_matrix(np.argmax(test_labels, axis=1), y_pred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)  # You'll need to define AA_andEachClassAccuracy function\n",
        "\n",
        "    kappa = cohen_kappa_score(np.argmax(test_labels, axis=1), y_pred)\n",
        "    score = model.evaluate(test_data, test_labels, batch_size=32)\n",
        "    test_loss = score[0] * 100\n",
        "    test_accuracy = score[1] * 100\n",
        "\n",
        "    return classification, confusion, test_loss, test_accuracy, oa * 100, each_acc * 100, aa * 100, kappa * 100, target_names, y_pred, total_time\n",
        "## Assigning Class Labels and Computing the Test Accuracy\n",
        "def reports(Te,TeC):\n",
        "    start = time.time()\n",
        "    Y_pred = model.predict(Te)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "    end = time.time()\n",
        "    total = end - start\n",
        "    target_names = ['Asphalt', 'Meadows', 'Gravel', 'Trees', 'Metal Sheet',\n",
        "                    'Bare Soil', 'Bitumen', 'Brick', 'Shadow']\n",
        "    classification = classification_report(np.argmax(TeC, axis=1), y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(np.argmax(TeC, axis=1), y_pred)\n",
        "    confusion = confusion_matrix(np.argmax(TeC, axis=1), y_pred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(np.argmax(TeC, axis=1), y_pred)\n",
        "    score = model.evaluate(Te, TeC, batch_size=32)\n",
        "    Test_Loss =  score[0]*100\n",
        "    Test_accuracy = score[1]*100\n",
        "    return classification, confusion, Test_Loss, Test_accuracy, oa*100, each_acc*100, aa*100, kappa*100, target_names, y_pred, total\n",
        "\n",
        "## Compute the Patch to Prepare for Ground Truths\n",
        "def Patch(data,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "    patch = data[height_slice, width_slice, :]\n",
        "    return patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Kv8-gieMCvW",
        "outputId": "052a783e-de4f-4a8e-fb73-259963b59f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 11, 11, 20, 1)]   0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 9, 18, 32)     320       \n",
            "                                                                 \n",
            " tf_op_layer_ExpandDims (Ten  [(None, 11, 9, 18, 32, 1  0        \n",
            " sorFlowOpLayer)             )]                                  \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 11, 7, 16, 26, 8)  512       \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 11, 5, 14, 22, 16  5776      \n",
            "                             )                                   \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 11, 3, 12, 20, 32  13856     \n",
            "                             )                                   \n",
            "                                                                 \n",
            " conv3d_4 (Conv3D)           (None, 11, 1, 10, 18, 64  55360     \n",
            "                             )                                   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 126720)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               32440576  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,550,457\n",
            "Trainable params: 32,550,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 11977 samples, validate on 5133 samples\n",
            "Epoch 1/50\n",
            "11977/11977 [==============================] - 46s 4ms/sample - loss: 4.4490 - acc: 0.6216 - val_loss: 0.2987 - val_acc: 0.9073\n",
            "Epoch 2/50\n",
            "11977/11977 [==============================] - 26s 2ms/sample - loss: 0.3200 - acc: 0.8935 - val_loss: 0.0701 - val_acc: 0.9797\n",
            "Epoch 3/50\n",
            "11977/11977 [==============================] - 28s 2ms/sample - loss: 0.1310 - acc: 0.9574 - val_loss: 0.0343 - val_acc: 0.9914\n",
            "Epoch 4/50\n",
            "11977/11977 [==============================] - 28s 2ms/sample - loss: 0.0843 - acc: 0.9753 - val_loss: 0.0136 - val_acc: 0.9961\n",
            "Epoch 5/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0472 - acc: 0.9856 - val_loss: 0.0079 - val_acc: 0.9984\n",
            "Epoch 6/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0333 - acc: 0.9895 - val_loss: 0.0066 - val_acc: 0.9975\n",
            "Epoch 7/50\n",
            "11977/11977 [==============================] - 28s 2ms/sample - loss: 0.0259 - acc: 0.9919 - val_loss: 0.0068 - val_acc: 0.9973\n",
            "Epoch 8/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.0081 - val_acc: 0.9984\n",
            "Epoch 9/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0212 - acc: 0.9941 - val_loss: 0.0040 - val_acc: 0.9990\n",
            "Epoch 10/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0113 - acc: 0.9972 - val_loss: 0.0075 - val_acc: 0.9971\n",
            "Epoch 11/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0190 - acc: 0.9947 - val_loss: 0.0059 - val_acc: 0.9986\n",
            "Epoch 12/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0114 - acc: 0.9972 - val_loss: 0.0115 - val_acc: 0.9969\n",
            "Epoch 13/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0095 - acc: 0.9976 - val_loss: 0.0062 - val_acc: 0.9977\n",
            "Epoch 14/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 0.0131 - val_acc: 0.9955\n",
            "Epoch 15/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0109 - acc: 0.9964 - val_loss: 0.0063 - val_acc: 0.9981\n",
            "Epoch 16/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0130 - acc: 0.9977 - val_loss: 9.0588e-04 - val_acc: 0.9998\n",
            "Epoch 17/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0118 - acc: 0.9982 - val_loss: 0.0032 - val_acc: 0.9984\n",
            "Epoch 18/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0026 - val_acc: 0.9990\n",
            "Epoch 19/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0063 - acc: 0.9984 - val_loss: 0.0034 - val_acc: 0.9986\n",
            "Epoch 20/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0044 - acc: 0.9983 - val_loss: 0.0127 - val_acc: 0.9975\n",
            "Epoch 21/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0018 - val_acc: 0.9994\n",
            "Epoch 22/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0053 - acc: 0.9987 - val_loss: 0.0031 - val_acc: 0.9988\n",
            "Epoch 23/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.0062 - val_acc: 0.9988\n",
            "Epoch 24/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0035 - val_acc: 0.9984\n",
            "Epoch 25/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0082 - acc: 0.9981 - val_loss: 0.0041 - val_acc: 0.9986\n",
            "Epoch 26/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0043 - acc: 0.9988 - val_loss: 0.0107 - val_acc: 0.9967\n",
            "Epoch 27/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0167 - acc: 0.9962 - val_loss: 0.0047 - val_acc: 0.9990\n",
            "Epoch 28/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0148 - val_acc: 0.9967\n",
            "Epoch 29/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0041 - acc: 0.9989 - val_loss: 0.0029 - val_acc: 0.9990\n",
            "Epoch 30/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0032 - val_acc: 0.9988\n",
            "Epoch 31/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0024 - val_acc: 0.9990\n",
            "Epoch 32/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0041 - val_acc: 0.9986\n",
            "Epoch 33/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0040 - acc: 0.9985 - val_loss: 0.0081 - val_acc: 0.9984\n",
            "Epoch 34/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0069 - acc: 0.9981 - val_loss: 0.0139 - val_acc: 0.9959\n",
            "Epoch 35/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0071 - acc: 0.9982 - val_loss: 0.0085 - val_acc: 0.9973\n",
            "Epoch 36/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0194 - acc: 0.9954 - val_loss: 0.0132 - val_acc: 0.9969\n",
            "Epoch 37/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0133 - acc: 0.9963 - val_loss: 0.0073 - val_acc: 0.9973\n",
            "Epoch 38/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0073 - val_acc: 0.9984\n",
            "Epoch 39/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0031 - val_acc: 0.9992\n",
            "Epoch 40/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0068 - val_acc: 0.9986\n",
            "Epoch 41/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.0029 - val_acc: 0.9992\n",
            "Epoch 42/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0020 - val_acc: 0.9992\n",
            "Epoch 43/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0032 - val_acc: 0.9992\n",
            "Epoch 44/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0065 - val_acc: 0.9984\n",
            "Epoch 45/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0047 - acc: 0.9989 - val_loss: 0.0068 - val_acc: 0.9984\n",
            "Epoch 46/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0045 - val_acc: 0.9982\n",
            "Epoch 47/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0058 - val_acc: 0.9986\n",
            "Epoch 48/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.0059 - val_acc: 0.9981\n",
            "Epoch 49/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0063 - val_acc: 0.9981\n",
            "Epoch 50/50\n",
            "11977/11977 [==============================] - 27s 2ms/sample - loss: 0.0063 - acc: 0.9987 - val_loss: 0.0052 - val_acc: 0.9988\n",
            "1387.3371903896332\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#ACCURACY BARSE\n",
        "\n",
        "import scipy.io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Input, Conv2D, Conv3D, Flatten, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from keras import backend as K\n",
        "\n",
        "# Load Pavia University dataset\n",
        "HSI = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/PaviaU.mat')['paviaU']\n",
        "GT = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/PaviaU_gt.mat')['paviaU_gt']\n",
        "\n",
        "# Reduce Dimensionality (Assuming DL_Method and HSICubes functions are defined)\n",
        "HSI = DL_Method(HSI, numComponents=20)\n",
        "HSI, GT = HSICubes(HSI, GT, WinSize=11)\n",
        "\n",
        "# Split Train and Test sets\n",
        "Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=0.60, random_state=42)\n",
        "\n",
        "# Split Train and Validation\n",
        "Tr, Tv, TrC, TvC = train_test_split(Tr, TrC, test_size=0.30, random_state=42)\n",
        "\n",
        "# Reshape and preprocess data\n",
        "Tr = Tr.reshape(-1, 11, 11, 20, 1)\n",
        "TrC = np_utils.to_categorical(TrC)\n",
        "Tv = Tv.reshape(-1, 11, 11, 20, 1)\n",
        "TvC = np_utils.to_categorical(TvC)\n",
        "\n",
        "# Model Structure\n",
        "input_layer = Input((11, 11, 20, 1))\n",
        "\n",
        "# 2D Convolutional Layer\n",
        "conv_layer2d = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(input_layer)\n",
        "\n",
        "# Reshape the 2D output to 3D using Keras backend\n",
        "reshaped_layer = K.expand_dims(conv_layer2d, axis=-1)\n",
        "\n",
        "# 3D Convolutional Layers\n",
        "conv_layer3d_1 = Conv3D(filters=8, kernel_size=(3, 3, 7), activation='relu')(reshaped_layer)\n",
        "conv_layer3d_2 = Conv3D(filters=16, kernel_size=(3, 3, 5), activation='relu')(conv_layer3d_1)\n",
        "conv_layer3d_3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(conv_layer3d_2)\n",
        "conv_layer3d_4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer3d_3)\n",
        "\n",
        "# Flatten Layer\n",
        "flatten_layer = Flatten()(conv_layer3d_4)\n",
        "\n",
        "# Fully Connected Layers\n",
        "dense_layer1 = Dense(units=256, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "\n",
        "# Output Layer\n",
        "output_layer = Dense(units=9, activation='softmax')(dense_layer2)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "adam = Adam(lr=0.001, decay=1e-06)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "# Training and Fit the Model\n",
        "start = time.time()\n",
        "history = model.fit(x=Tr, y=TrC, batch_size=256, epochs=50, validation_data=(Tv, TvC))\n",
        "end = time.time()\n",
        "Tr_Time = end - start\n",
        "print(Tr_Time)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Input, Conv2D, Conv3D, Flatten, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import time\n",
        "from keras import backend as K\n",
        "\n",
        "# Load Pavia University dataset\n",
        "HSI = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/PaviaU.mat')['paviaU']\n",
        "GT = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/PaviaU_gt.mat')['paviaU_gt']\n",
        "\n",
        "# Assuming DL_Method and HSICubes functions are defined\n",
        "HSI = DL_Method(HSI, numComponents=20)\n",
        "HSI, GT = HSICubes(HSI, GT, WinSize=11)\n",
        "\n",
        "# Data splitting\n",
        "Tr, Tv, TrC, TvC = train_test_split(HSI, GT, test_size=0.40, random_state=42)\n",
        "Tr, Tv, TrC, TvC = train_test_split(Tr, TrC, test_size=0.30, random_state=42)\n",
        "\n",
        "# Data preprocessing\n",
        "Tr, Tv = Tr.reshape(-1, 11, 11, 20, 1), Tv.reshape(-1, 11, 11, 20, 1)\n",
        "TrC, TvC = np_utils.to_categorical(TrC), np_utils.to_categorical(TvC)\n",
        "\n",
        "# Model definition\n",
        "input_layer = Input((11, 11, 20, 1))\n",
        "conv_layer2d = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
        "reshaped_layer = K.expand_dims(conv_layer2d, -1)\n",
        "#conv_layer3d = [Conv3D(filters, (3, 3, k), activation='relu')(conv_layer3d) for filters, k in zip([8, 16, 32, 64], [7, 5, 3, 3])]\n",
        "flatten_layer = Flatten()(conv_layer3d[-1])\n",
        "dense_layer = [Dense(units, activation='relu')(flatten_layer) for units in [256, 128]]\n",
        "dense_layer = [Dropout(0.4)(dense) for dense in dense_layer]\n",
        "output_layer = Dense(9, activation='softmax')(dense_layer[-1])\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001, decay=1e-06), metrics=['accuracy'])\n",
        "model.summary()\n",
        "# Model training\n",
        "start = time.time()\n",
        "history = model.fit(x=Tr, y=TrC, batch_size=256, epochs=50, validation_data=(Tv, TvC))\n",
        "end = time.time()\n",
        "Tr_Time = end - start\n",
        "print(Tr_Time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "cFRgmdfjqYZk",
        "outputId": "2101b637-aff5-4f42-9b8f-0f68f180a368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-755a70669de2>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mreshaped_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_layer2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#conv_layer3d = [Conv3D(filters, (3, 3, k), activation='relu')(conv_layer3d) for filters, k in zip([8, 16, 32, 64], [7, 5, 3, 3])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mflatten_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_layer3d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mdense_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_layer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0munits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdense_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdense\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdense_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conv_layer3d' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#98.65%\n",
        "!pip install matplotlib==2.*"
      ],
      "metadata": {
        "id": "YUQuunUeTY3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot Training and Validation loss and Accuracy\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "plt.grid()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Training and Validation loss')\n",
        "plt.legend(['Training','Validation'], loc='upper right')\n",
        "#plt.savefig(\"IP_loss.eps\")\n",
        "plt.show()\n",
        "## Plot Training and Validation Accuracy\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.ylim(0,1.1)\n",
        "plt.grid()\n",
        "plt.plot(history.history['acc'])\n",
        "#plt.plot(history.history['accuracy'])\n",
        "#plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend(['Training','Validation'])\n",
        "#plt.savefig(\"IP_Accuracy.eps\")\n",
        "plt.show()\n",
        "## Testing Phase\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
        "## Reshape Test Data\n",
        "Te = Te.reshape(-1, 11, 11, 20, 1)\n",
        "TeC = np_utils.to_categorical(TeC)\n",
        "Te.shape, TeC.shape\n",
        "## Computing and Writing the Accuacy in .txt file\n",
        "classification,confusion,Test_loss,Test_accuracy,oa,each_acc,aa,kappa,target_names,y_pred,Time = reports(Te,TeC)\n",
        "classification = str(classification)\n",
        "confusion = str(confusion)\n",
        "print(classification), print(Test_accuracy), print(oa), print(aa), print(Tr_Time), print(Time)\n",
        "## Draw Confusion Matrix\n",
        "confusion = confusion_matrix(np.argmax(TeC, axis=1), y_pred, labels=np.unique(np.argmax(TeC, axis=1)))\n",
        "cm_sum = np.sum(confusion, axis=1, keepdims=True)\n",
        "cm_perc = confusion / cm_sum.astype(float) * 100\n",
        "annot = np.empty_like(confusion).astype(str)\n",
        "nrows, ncols = confusion.shape\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        c = confusion[i, j]\n",
        "        p = cm_perc[i, j]\n",
        "        if i == j:\n",
        "            s = cm_sum[i]\n",
        "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "        elif c == 0:\n",
        "            annot[i, j] = ''\n",
        "        else:\n",
        "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "cm = pd.DataFrame(confusion, index=np.unique(target_names), columns=np.unique(target_names))\n",
        "cm.index.name = 'Actual'\n",
        "cm.columns.name = 'Predicted'\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
        "## Loading Dataset\n",
        "data_path = os.path.join(os.getcwd(),'/content/drive/My Drive/Colab')\n",
        "HSI = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/PaviaU.mat')['paviaU']\n",
        "GT = scipy.io.loadmat('/content/drive/MyDrive/Colab Notebooks/PaviaU_gt.mat')['paviaU_gt']\n",
        "\n",
        "## Check the Dimensions of HSI\n",
        "height = GT.shape[0]\n",
        "width = GT.shape[1]\n",
        "PATCH_SIZE = 11\n",
        "## Dimensional Reduction and zero padding\n",
        "HSI = DL_Method(HSI, numComponents=20)\n",
        "HSI.shape\n",
        "HSI = ZeroPad(HSI, 11//2)\n",
        "## Calculate the predicted Ground Truths\n",
        "outputs = np.zeros((height,width))\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        target = int(GT[i,j])\n",
        "        if target == 0 :\n",
        "            continue\n",
        "        else :\n",
        "            image_patch=Patch(HSI,i,j)\n",
        "            X_test_image = image_patch.reshape(1,image_patch.shape[0],\n",
        "                                               image_patch.shape[1], image_patch.shape[2], 1).astype('float32')\n",
        "            prediction = (model.predict(X_test_image))\n",
        "            prediction = np.argmax(prediction, axis=1)\n",
        "            outputs[i][j] = prediction+1\n",
        "## Show Ground Truths\n",
        "ground_truth = spectral.imshow(classes = GT,figsize =(7,7))\n",
        "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(7,7))\n",
        "spectral.save_rgb(\"predictions.png\", outputs.astype(int), colors=spectral.spy_colors)\n",
        "spectral.save_rgb(\"ground_truth.png\", GT, colors=spectral.spy_colors)"
      ],
      "metadata": {
        "id": "HgrWyTKNFvUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and Print Average Accuracy\n",
        "average_accuracy = np.mean(history.history['val_acc'])\n",
        "print(f\"Average Validation Accuracy: {average_accuracy:.4f}\")\n",
        "y_pred = model.predict(Tv)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "true_labels = np.argmax(TvC, axis=1)\n",
        "\n",
        "kappa = cohen_kappa_score(true_labels, y_pred_labels)\n",
        "print(f\"Kappa Score: {kappa:.4f}\")\n",
        "\n",
        "y_pred = model.predict(Tv)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "true_labels = np.argmax(TvC, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(true_labels, y_pred_labels)\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "dXa-JuyIdsg-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOzCGaNRKpkbPalZvhyK2Zx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}